{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Transfer learning**","metadata":{}},{"cell_type":"markdown","source":"## **import needed modules**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom torch import nn,optim\nfrom torchvision import transforms,datasets,models\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n\n\nimport os\n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T17:54:28.792118Z","iopub.execute_input":"2021-11-29T17:54:28.792423Z","iopub.status.idle":"2021-11-29T17:54:30.387946Z","shell.execute_reply.started":"2021-11-29T17:54:28.792344Z","shell.execute_reply":"2021-11-29T17:54:30.387152Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## **training and test datasets**","metadata":{}},{"cell_type":"markdown","source":"#### **do some transforms**","metadata":{}},{"cell_type":"code","source":"# all pretrained models expects at least 224x224 RGB image\n# also, it s needed normalize images with mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n#we rotate images randomly upto 30 degrees\n#we randomly flip images horizontally with probability of 0.5\n#this augumentations help us to increase robustness\ntrain_transform = transforms.Compose([transforms.Resize((224,224)),\n                                    transforms.RandomRotation(30),\n                                    transforms.RandomHorizontalFlip(p=0.5),\n                                    transforms.ToTensor(),\n                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                        std=[0.229, 0.224, 0.225])])\n#in test, we dont need to rotate or flip the images \ntest_transform = transforms.Compose([transforms.Resize((224,224)),\n                                    transforms.ToTensor(),\n                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                        std=[0.229, 0.224, 0.225])])\n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T17:54:30.389803Z","iopub.execute_input":"2021-11-29T17:54:30.390068Z","iopub.status.idle":"2021-11-29T17:54:30.396882Z","shell.execute_reply.started":"2021-11-29T17:54:30.390032Z","shell.execute_reply":"2021-11-29T17:54:30.396202Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"#### **datasets and loaders**","metadata":{}},{"cell_type":"code","source":"root_path = '../input/dogs-cats-images/dataset/'\n\n#training\ntrainset = datasets.ImageFolder(os.path.join(root_path,'training_set'),transform=train_transform)\ntrainloader = DataLoader(trainset,batch_size=64,shuffle=True)\n\n#test\ntestnset = datasets.ImageFolder(os.path.join(root_path,'test_set'),transform=test_transform)\ntestloader = DataLoader(trainset,batch_size=64)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T17:54:30.398249Z","iopub.execute_input":"2021-11-29T17:54:30.398771Z","iopub.status.idle":"2021-11-29T17:54:31.793670Z","shell.execute_reply.started":"2021-11-29T17:54:30.398731Z","shell.execute_reply":"2021-11-29T17:54:31.792940Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## **Load desired pretrained model**\nhttps://pytorch.org/vision/stable/models.html","metadata":{}},{"cell_type":"code","source":"model = models.densenet121(pretrained=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T17:54:31.796387Z","iopub.execute_input":"2021-11-29T17:54:31.796889Z","iopub.status.idle":"2021-11-29T17:54:37.102898Z","shell.execute_reply.started":"2021-11-29T17:54:31.796850Z","shell.execute_reply":"2021-11-29T17:54:37.102143Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"#### **as you can see this model immensely huge**","metadata":{}},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2021-11-29T17:54:37.104121Z","iopub.execute_input":"2021-11-29T17:54:37.104895Z","iopub.status.idle":"2021-11-29T17:54:37.117403Z","shell.execute_reply.started":"2021-11-29T17:54:37.104854Z","shell.execute_reply":"2021-11-29T17:54:37.116562Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"#### **depending on our task, we can freeze all convolutional layer and or part of it. Then we retrain the rest.**\n#### **in both cases, we need to know the input dimension of our future layer**\n#### **densenet121, cosists of conv. layer(feautures) and fully connected layer(classifier), we can access its characteristics**","metadata":{}},{"cell_type":"code","source":"model.classifier.in_features","metadata":{"execution":{"iopub.status.busy":"2021-11-29T17:54:37.118783Z","iopub.execute_input":"2021-11-29T17:54:37.119217Z","iopub.status.idle":"2021-11-29T17:54:37.213339Z","shell.execute_reply.started":"2021-11-29T17:54:37.119183Z","shell.execute_reply":"2021-11-29T17:54:37.212550Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## **freeze parameters**","metadata":{}},{"cell_type":"markdown","source":"#### freeze","metadata":{}},{"cell_type":"code","source":"#we can freeze using requires_grad=False\nfor param in model.features.parameters():\n    param.requires_grad=False\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T17:54:37.215045Z","iopub.execute_input":"2021-11-29T17:54:37.215355Z","iopub.status.idle":"2021-11-29T17:54:37.223989Z","shell.execute_reply.started":"2021-11-29T17:54:37.215318Z","shell.execute_reply":"2021-11-29T17:54:37.223222Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"#### initilize learning layers","metadata":{}},{"cell_type":"code","source":"# we can access input dimension using a line of code below:\ninput_dim = model.classifier.in_features\n#for denseNet121 fully connected layers, it is 1024\ninput_dim","metadata":{"execution":{"iopub.status.busy":"2021-11-29T17:54:37.225315Z","iopub.execute_input":"2021-11-29T17:54:37.225810Z","iopub.status.idle":"2021-11-29T17:54:37.234035Z","shell.execute_reply.started":"2021-11-29T17:54:37.225773Z","shell.execute_reply":"2021-11-29T17:54:37.232852Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"hidden = [512,128]\nout_dim = 2 # our task is binary classification, so it s 2\nLearningNet = nn.Sequential(OrderedDict([\n    ('cf1',nn.Linear(input_dim,hidden[0])),\n    ('act1',nn.ReLU()),\n    ('cf2',nn.Linear(hidden[0],hidden[1])),\n    ('act2',nn.ReLU()),\n    ('cf3',nn.Linear(hidden[1],out_dim)),\n    ('output',nn.Softmax(dim=1))\n]))\nmodel.classifier = LearningNet\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T17:54:37.235433Z","iopub.execute_input":"2021-11-29T17:54:37.235923Z","iopub.status.idle":"2021-11-29T17:54:37.249950Z","shell.execute_reply.started":"2021-11-29T17:54:37.235849Z","shell.execute_reply":"2021-11-29T17:54:37.249106Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"#### model is ready to learn","metadata":{}},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:04:38.226007Z","iopub.execute_input":"2021-11-29T18:04:38.226277Z","iopub.status.idle":"2021-11-29T18:04:38.229749Z","shell.execute_reply.started":"2021-11-29T18:04:38.226249Z","shell.execute_reply":"2021-11-29T18:04:38.228994Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## **train loop**","metadata":{}},{"cell_type":"code","source":"def train(model,trainloader,testloader,criterion,epochs,lr=0.3,print_every=50):\n    max_val_acc = 0\n    optimizer = optim.Adam(model.parameters(),lr=lr)\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n    for epoch in range(epochs):\n        tr_acc_epoch = []\n        tr_loss_epoch = []\n        val_acc_epoch = []\n        val_loss_epoch = []\n        tr_losses = 0\n        val_losses = 0\n        tr_accs = 0\n        val_accs = 0\n        print(f'running {epoch+1} out of {epochs} epochs')\n        #training\n        model.train()\n        for i,(img_tr,y_tr) in enumerate(iter(trainloader)):\n            #passing input and label to the device(cuda if available, else cpu)\n            img_tr = img_tr.to(device)\n            y_tr = y_tr.to(device)\n\n            #setting grad to zero, preventing gradient accumulation\n            optimizer.zero_grad()\n\n            #prediction and loss calc\n            pred_tr = model.forward(img_tr)\n            loss_tr = criterion(pred_tr,y_tr)\n\n            #backprop\n            loss_tr.backward()\n            optimizer.step()\n\n            #loss metrics\n            tr_losses+=loss_tr.item()\n            #accuracy metrics\n            tr_accs += y_tr.eq(pred_tr.detach().argmax(dim=1)).float().mean().item()\n\n            #printing metrics per predefined interval\n            if (i+1) % print_every==0:\n                print(f'\\t\\t iter {i+1}-training loss:  {(tr_losses/print_every):.3f},\\ttraining accuracy:  {(tr_accs/print_every):.3f} ')\n                tr_loss_epoch.append(tr_losses/print_every)\n                tr_acc_epoch.append(tr_accs/print_every)\n                tr_losses = 0\n                tr_accs= 0\n        #validation\n        model.eval()\n        with torch.no_grad():\n            for j,(img_val,y_val) in enumerate(iter(testloader)):\n                #passing input and label to device \n                img_val = img_val.to(device)\n                y_val = y_val.to(device)\n\n                #prediction and loss calc\n                pred_val = model.forward(img_val)\n                loss_val = criterion(pred_val,y_val)\n\n                #loss metrics\n                val_losses+=loss_val.item()\n                #accuracy metrics\n                val_accs += y_val.eq(pred_val.detach().argmax(dim=1)).float().mean().item()\n\n                #printing metrics per predefined interval\n                if (j+1) % print_every==0:\n                    print(f'\\t\\t iter {j+1} - valid. loss:  {(val_losses/print_every):.3f},\\taccuracy:  {(val_accs/print_every):.3f} ')\n                    val_loss_epoch.append(val_losses/print_every)\n                    val_acc_epoch.append(val_accs/print_every)\n                    val_losses = 0\n                    val_accs= 0\n        avg_val_acc = np.mean(val_acc_epoch)\n        print(\"---\"*30)\n        print(f'\\t epoch {epoch+1}- train loss:  {np.mean(tr_loss_epoch)},\\taccuracy:  {np.mean(tr_acc_epoch)}')\n        print(f'\\t epoch {epoch+1}- validation loss:  {np.mean(val_loss_epoch)},\\taccuracy:  {avg_val_acc}')\n        print(\"---\"*30)\n        \n        #saving the model, accuracy will be checked, if it 's greater than max, we ll save model's params\n        if avg_val_acc>max_val_acc:\n            torch.save(model.state_dict(),'checkpoint.pth')\n            max_val_acc = avg_val_acc\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:35:32.523351Z","iopub.execute_input":"2021-11-29T18:35:32.523637Z","iopub.status.idle":"2021-11-29T18:35:32.540307Z","shell.execute_reply.started":"2021-11-29T18:35:32.523584Z","shell.execute_reply":"2021-11-29T18:35:32.539378Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### **start training**","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nlr=0.003\nepochs = 3 \nprint_interval =50\ntrain(model,trainloader,testloader,criterion,epochs,lr,print_interval)\n","metadata":{"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Loading saved model","metadata":{}},{"cell_type":"markdown","source":"in training loop, after every epoch we saved parameters of our model. It can be helpful, if we dont want lose our training time and efforts. Or to save the best performance parameters, then we can break our training loop if our model stops learning.","metadata":{}},{"cell_type":"markdown","source":"* **to save model** torch.save(model.state_dict, 'file_name.pth')\nlater we can load parameters to the skeleton of our model.\n* **to load saved parameters** \nparams = torch.load('filename.pth') -> model.load_state_dict(params)\n","metadata":{}},{"cell_type":"code","source":"state_dict  = torch.load('checkpoint.pth')\nmodel.load_state_dict(state_dict)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:21:55.235933Z","iopub.execute_input":"2021-11-29T18:21:55.236232Z","iopub.status.idle":"2021-11-29T18:21:55.480242Z","shell.execute_reply.started":"2021-11-29T18:21:55.236199Z","shell.execute_reply":"2021-11-29T18:21:55.479632Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## continue training","metadata":{}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\nlr=0.002\nepochs = 10\nprint_interval = 50\ntrain(model,trainloader,testloader,criterion,epochs,lr,print_interval)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:35:36.495583Z","iopub.execute_input":"2021-11-29T18:35:36.496397Z","iopub.status.idle":"2021-11-29T18:53:53.956285Z","shell.execute_reply.started":"2021-11-29T18:35:36.496354Z","shell.execute_reply":"2021-11-29T18:53:53.955633Z"},"trusted":true},"execution_count":26,"outputs":[]}]}