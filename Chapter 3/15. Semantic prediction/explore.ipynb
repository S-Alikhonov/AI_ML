{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Amazon Reviews sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "import  torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **loading dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = datasets.AmazonReviewFull(root='data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/AmazonReviewFull/amazon_review_full_csv/train.csv',nrows=500,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/AmazonReviewFull/amazon_review_full_csv/test.csv',nrows=100,header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **merging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['reviews'] = train.iloc[:,1]+ ' ' +train.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>more like funchuck</td>\n",
       "      <td>Gave this to my dad for a gag gift after direc...</td>\n",
       "      <td>more like funchuck Gave this to my dad for a g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Inspiring</td>\n",
       "      <td>I hope a lot of people hear this cd. We need m...</td>\n",
       "      <td>Inspiring I hope a lot of people hear this cd....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>The best soundtrack ever to anything.</td>\n",
       "      <td>I'm reading a lot of reviews saying that this ...</td>\n",
       "      <td>The best soundtrack ever to anything. I'm read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Chrono Cross OST</td>\n",
       "      <td>The music of Yasunori Misuda is without questi...</td>\n",
       "      <td>Chrono Cross OST The music of Yasunori Misuda ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Too good to be true</td>\n",
       "      <td>Probably the greatest soundtrack in history! U...</td>\n",
       "      <td>Too good to be true Probably the greatest soun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>2</td>\n",
       "      <td>not really worth it</td>\n",
       "      <td>I wore this for my wedding since I hadn't lost...</td>\n",
       "      <td>not really worth it I wore this for my wedding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>2</td>\n",
       "      <td>Painful Experience</td>\n",
       "      <td>I think the concept is great but with this par...</td>\n",
       "      <td>Painful Experience I think the concept is grea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>3</td>\n",
       "      <td>It works but it hurts</td>\n",
       "      <td>It sucked all of me in &amp; I looked great. I wor...</td>\n",
       "      <td>It works but it hurts It sucked all of me in &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>3</td>\n",
       "      <td>not what I expected</td>\n",
       "      <td>Based on the positive reviews, I was excited t...</td>\n",
       "      <td>not what I expected Based on the positive revi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>1</td>\n",
       "      <td>poor fit</td>\n",
       "      <td>After trying three different sizes,I still cou...</td>\n",
       "      <td>poor fit After trying three different sizes,I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0                                      1  \\\n",
       "0    3                     more like funchuck   \n",
       "1    5                              Inspiring   \n",
       "2    5  The best soundtrack ever to anything.   \n",
       "3    4                       Chrono Cross OST   \n",
       "4    5                    Too good to be true   \n",
       "..  ..                                    ...   \n",
       "495  2                    not really worth it   \n",
       "496  2                     Painful Experience   \n",
       "497  3                  It works but it hurts   \n",
       "498  3                    not what I expected   \n",
       "499  1                               poor fit   \n",
       "\n",
       "                                                     2  \\\n",
       "0    Gave this to my dad for a gag gift after direc...   \n",
       "1    I hope a lot of people hear this cd. We need m...   \n",
       "2    I'm reading a lot of reviews saying that this ...   \n",
       "3    The music of Yasunori Misuda is without questi...   \n",
       "4    Probably the greatest soundtrack in history! U...   \n",
       "..                                                 ...   \n",
       "495  I wore this for my wedding since I hadn't lost...   \n",
       "496  I think the concept is great but with this par...   \n",
       "497  It sucked all of me in & I looked great. I wor...   \n",
       "498  Based on the positive reviews, I was excited t...   \n",
       "499  After trying three different sizes,I still cou...   \n",
       "\n",
       "                                               reviews  \n",
       "0    more like funchuck Gave this to my dad for a g...  \n",
       "1    Inspiring I hope a lot of people hear this cd....  \n",
       "2    The best soundtrack ever to anything. I'm read...  \n",
       "3    Chrono Cross OST The music of Yasunori Misuda ...  \n",
       "4    Too good to be true Probably the greatest soun...  \n",
       "..                                                 ...  \n",
       "495  not really worth it I wore this for my wedding...  \n",
       "496  Painful Experience I think the concept is grea...  \n",
       "497  It works but it hurts It sucked all of me in &...  \n",
       "498  not what I expected Based on the positive revi...  \n",
       "499  poor fit After trying three different sizes,I ...  \n",
       "\n",
       "[500 rows x 4 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **dropping other merged individual columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop([1,2],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **encoding** labels(1-5) -> (0-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some ordinal encoding on labels\n",
    "train[0] = train[0].apply(lambda x: x-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 4, 3, 0, 1])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get spacy work done**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x7f9fa9c4cc40>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **tokenization, lemmatization, punctuation removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_punct and not token.is_stop]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['well', 'version', 'little', 'bit', 'harsh', 'come', 'prep']\n"
     ]
    }
   ],
   "source": [
    "text = 'It can be done better than this, because this version little bit harsh when it comes to prep'\n",
    "print(prep(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **textual prep. is almost done, yet word 2 vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext.vocab import FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_indexer = FastText('simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5160"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fast text has vocab, it can map words to indexes in that vocab\n",
    "#we cleaned text, got token lemmatized, now we can get indexes for each word\n",
    "word_indexer.stoi['chicken']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # but there is a case when word is not existed in that vocab\n",
    "# word_indexer.stoi('Saidalikhon')\n",
    "# #we should handle it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**token endcoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_encoder(token,word_indexer):\n",
    "    if token == '<pad>':\n",
    "        return 1\n",
    "    else:\n",
    "        try:\n",
    "            return word_indexer.stoi[token]\n",
    "        except:\n",
    "            if type(token) != str :\n",
    "                print(f'expected str, but got {type(token)} instead.')\n",
    "            else:\n",
    "                return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_encoder('Saidalikhon',word_indexer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**text encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_encoder(tokens,word_indexer):\n",
    "    '''\n",
    "    input - list of lemmatized tokens\n",
    "    returns - list of encoded tokens\n",
    "    '''\n",
    "    return [token_encoder(token,word_indexer) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[624, 216, 1044, 0, 13200, 178, 6975, 220]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'It might be the case , when you got job done in Pytorch, then realise there is a better and efficient way of it.'\n",
    "text_encoder(prep(text),word_indexer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**padding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have reviews, with different length\n",
    "#but our model expects the same dimensionality across its life cycle\n",
    "#so we should add padding(if review has less amount word than max_length)\n",
    "#or we should slice the review if it has more than max_length\n",
    "# 5 * [1] -> [1,1,1,1,1], 0 * [1] -> [], -x * [1] -> []\n",
    "def padding(list_indexed,max_length,pad=1):\n",
    "    #padding short reviews\n",
    "    res = list_indexed + (max_length - len(list_indexed))*[pad]\n",
    "    # slicing prior to return, if review is longer\n",
    "    return res[:max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[624, 216, 1044, 0, 13200, 178, 6975, 220, 1, 1]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'It might be the case , when you got job done in Pytorch, then realise there is a better and efficient way of it.'\n",
    "padding(text_encoder(prep(text),word_indexer),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[624, 216, 1044, 0, 13200]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding(text_encoder(prep(text),word_indexer),5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**splitting into train and val**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train['reviews'].values\n",
    "y=train[0].values\n",
    "x_train,x_val,y_train,y_val = train_test_split(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,x,y,max_length=32):\n",
    "        self.max_length = max_length\n",
    "        self.vec = FastText('simple')\n",
    "        self.vec.vectors[0] = torch.zeros(self.vec.vectors[0].shape[0])\n",
    "        self.vec.vectors[0] = -torch.ones(self.vec.vectors[0].shape[0])\n",
    "        self.labels = y\n",
    "        self.inputs = [padding(text_encoder(prep(review),self.vec),self.max_length) for review in x]\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        assert len(self.inputs[i]) == self.max_length\n",
    "        return self.inputs[i],self.labels[i]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we create our custom collate function, later use inside dataloader, as collate function to batchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each item of the the set, contains (input,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'CustomDataset.__init__.<locals>.<lambda>'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1h/x7tmv9xd1bsclx868ql32nf40000gn/T/ipykernel_3346/375394626.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdataloader_tr2save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_tr2save\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdataloader_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_ENV/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/NLP_ENV/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0mpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m     \u001b[0mdata_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_buf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'CustomDataset.__init__.<locals>.<lambda>'"
     ]
    }
   ],
   "source": [
    "trainset = CustomDataset(x_train,y_train)\n",
    "def collate(batch,vectorizer=FastText('simple').vectors):\n",
    "    #inner torch.stack is stacking vectorized words into review tensor\n",
    "    #outer torch.stack is stacking that review into batch tensor\n",
    "    inputs = torch.stack([torch.stack([vectorizer[token] for token in item[0]]) for item in batch])\n",
    "    #\n",
    "    #converting labels into Long type tensors, as criterion functions expects that dtype\n",
    "    labels = torch.LongTensor([item[1] for item in batch])\n",
    "    return inputs, labels\n",
    "dataloader_tr2save = DataLoader(trainset,batch_size=64,collate_fn=collate)\n",
    "torch.save(dataloader_tr2save,'train.pth')\n",
    "dataloader_tr = torch.load('train.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valset = CustomDataset(x_val,y_val)\n",
    "dataloader_val2save = DataLoader(valset,batch_size=64,collate_fn=collate)\n",
    "torch.save(dataloader_val2save,'test.pth')\n",
    "dataloader_val = torch.load('test.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim =300\n",
    "class Model(nn.Module):\n",
    "    def __init__(self,hidden1,hidden2,out_dim,max_length=32):\n",
    "        super(Model,self).__init__()\n",
    "        self.fc = nn.Linear(max_length*embed_dim,hidden1)\n",
    "        self.fc2 = nn.Linear(hidden1,hidden2)\n",
    "        self.fc3 = nn.Linear(hidden2,out_dim)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        z = F.relu(self.fc(x))\n",
    "        z = F.relu(self.fc2(z))\n",
    "        z = F.log_softmax(self.fc3(z),dim=1)\n",
    "\n",
    "        return z\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running 1 out of 2 epochs \n",
      "\trunning train iteration 100\n",
      "\t\ttrain loss : 1.546\t accuracy : 0.002\n",
      "\trunning train iteration 200\n",
      "\t\ttrain loss : 1.478\t accuracy : 0.004\n",
      "\trunning train iteration 300\n",
      "\t\ttrain loss : 1.444\t accuracy : 0.010\n",
      "\trunning train iteration 400\n",
      "\t\ttrain loss : 1.427\t accuracy : 0.004\n",
      "\trunning train iteration 500\n",
      "\t\ttrain loss : 1.421\t accuracy : 0.006\n",
      "\trunning train iteration 600\n",
      "\t\ttrain loss : 1.406\t accuracy : 0.002\n",
      "train loss : 1.4536795826752982\t accuracy : 0.004609375\n",
      "\trunning validation iteration 100\n",
      "\t\tvalidation loss : 1.411\t accuracy : 0.002\n",
      "** train loss : 1.4105382359027863\t accuracy : 0.00171875 **\n",
      "running 2 out of 2 epochs \n",
      "\trunning train iteration 100\n",
      "\t\ttrain loss : 1.335\t accuracy : 0.002\n",
      "\trunning train iteration 200\n",
      "\t\ttrain loss : 1.304\t accuracy : 0.002\n",
      "\trunning train iteration 300\n",
      "\t\ttrain loss : 1.294\t accuracy : 0.000\n",
      "\trunning train iteration 400\n",
      "\t\ttrain loss : 1.286\t accuracy : 0.002\n",
      "\trunning train iteration 500\n",
      "\t\ttrain loss : 1.291\t accuracy : 0.004\n",
      "\trunning train iteration 600\n",
      "\t\ttrain loss : 1.292\t accuracy : 0.008\n",
      "train loss : 1.3000988638401032\t accuracy : 0.0029947916666666664\n",
      "\trunning validation iteration 100\n",
      "\t\tvalidation loss : 1.424\t accuracy : 0.004\n",
      "** train loss : 1.4244003367424012\t accuracy : 0.0040625 **\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "model = Model(100,50,5)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = Adam(model.parameters(),lr=0.003)\n",
    "epochs = 2\n",
    "pr = 100\n",
    "for epoch in range(epochs):\n",
    "    losses_val = []\n",
    "    losses_tr = []\n",
    "    accs_val = []\n",
    "    accs_tr = []\n",
    "    acc_tr = 0\n",
    "    acc_val = 0\n",
    "    loss_tr = 0\n",
    "    loss_val = 0\n",
    "\n",
    "    print(f'running {epoch+1} out of {epochs} epochs ')\n",
    "    model.train()\n",
    "    for i,(x_tr,y_tr) in enumerate(iter(dataloader_tr)):\n",
    "        optimizer.zero_grad()\n",
    "        pred_tr = model.forward(x_tr.view(x_tr.shape[0],-1))\n",
    "        pred_tr_ps = torch.exp(pred_tr.detach())\n",
    "        loss_t = criterion(pred_tr,y_tr)\n",
    "        loss_t.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #metrics\n",
    "        loss_tr += loss_t.item()\n",
    "        acc_tr += y_tr.eq(pred_tr_ps.argmax()).float().mean().item()\n",
    "        if (i+1) % pr == 0 :\n",
    "            print(f'\\trunning train iteration {i+1}')\n",
    "            print(f'\\t\\ttrain loss : {loss_tr/pr:.3f}\\t accuracy : {acc_tr/pr:.3f}')\n",
    "            losses_tr.append(loss_tr/pr)\n",
    "            accs_tr.append(acc_tr/pr)\n",
    "            loss_tr = 0\n",
    "            acc_tr = 0\n",
    "    print(f'train loss : {np.mean(losses_tr)}\\t accuracy : {np.mean(accs_tr)}')\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for j,(x_val,y_val) in enumerate(iter(dataloader_val)):\n",
    "            pred_val = model.forward(x_val.view(x_val.shape[0],-1))\n",
    "            pred_val_ts = torch.exp(pred_val.detach())\n",
    "            loss_v = criterion(pred_val,y_val)\n",
    "            #metrics\n",
    "            loss_val += loss_v.item()\n",
    "            acc_val += y_val.eq(torch.exp(pred_val).detach().argmax()).float().mean().item()\n",
    "            if (j+1) % pr == 0 :\n",
    "                print(f'\\trunning validation iteration {j+1}')\n",
    "                print(f'\\t\\tvalidation loss : {loss_val/pr:.3f}\\t accuracy : {acc_val/pr:.3f}')\n",
    "                losses_val.append(loss_val/pr)\n",
    "                accs_val.append(acc_val/pr)\n",
    "                loss_val = 0\n",
    "                acc_val = 0\n",
    "        print(f'** train loss : {np.mean(losses_val):.3f}\\t accuracy : {np.mean(accs_val):3f} **')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32d51a5fea8c6d96b3ba385d558c569b949285f78f32378af2035738b4398847"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('NLP_ENV': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
