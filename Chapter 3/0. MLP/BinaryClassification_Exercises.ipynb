{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:47:37.551793Z",
     "start_time": "2021-05-26T07:47:35.439944Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# create a neural network class inheriting from the nn.Module\n",
    "# Call it NeuralNetwork and make, and use \"pass\" in the constructor\n",
    "# so that it doesn't give an error\n",
    "# Instantiate one instance of it in variable net\n",
    "\n",
    "net = 0\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self ):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        pass\n",
    "\n",
    "net = NeuralNetwork()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:50:32.482086Z",
     "start_time": "2021-05-26T07:50:32.456893Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "assert isinstance(net, NeuralNetwork)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:51:28.420569Z",
     "start_time": "2021-05-26T07:51:28.412916Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# Rewrite the NeuralNetwork class so that the constructor receives\n",
    "# as input the input_dim and num_hidden, respectively the dimension of \n",
    "# the input and the number of hidden neurons\n",
    "# use pass again\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    pass\n",
    "    def __init__(self, input_dim, num_hidden):\n",
    "        super().__init__()\n",
    "        \n"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:56:11.203531Z",
     "start_time": "2021-05-26T07:56:11.199729Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "assert NeuralNetwork(input_dim=10, num_hidden=16)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:56:32.252906Z",
     "start_time": "2021-05-26T07:56:32.247913Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# Rewrite the NeuralNetwork class so that the constructor receives\n",
    "# as input the input_dim, num_hidden1 and num_hidden2, respectively the dimension of \n",
    "# the input and the number of hidden neurons for the first fully connected\n",
    "# layer and the second. Define the attributes in the constructor\n",
    "# that consists of the layers, call them fc1, fc2 and fc3 and a sigmoid.\n",
    "# use pass again. Be careful to put the dimensions in the right places!\n",
    "# Since we will do a binary classification problem, fc3 will have 1 neuron\n",
    "# as output\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_hidden1, num_hidden2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim,num_hidden1)\n",
    "        self.fc2 = nn.Linear(num_hidden1,num_hidden2)\n",
    "        self.fc3 = nn.Linear(num_hidden2,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        l1 = self.fc1(x)\n",
    "        activation1 = self.sigmoid(l1)\n",
    "        l2 = self.fc2(activation1)\n",
    "        activation2 = self.sigmoid(l2)\n",
    "        l3 = self.fc3(activation2)\n",
    "        output = self.sigmoid(l3)\n",
    "\n",
    "        return output\n",
    "\n",
    "        "
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T08:04:27.491588Z",
     "start_time": "2021-05-26T08:04:27.484159Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "net = NeuralNetwork(16, 16, 16)\n",
    "assert net.fc1\n",
    "assert net.fc2\n",
    "assert net.fc3\n",
    "assert net.sigmoid"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T08:04:48.612004Z",
     "start_time": "2021-05-26T08:04:48.606773Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# Define the forward pass to make a reasonable use of the attributes\n",
    "# you defined before. Follow the same reasoning we used in class\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# For training a model, use the following optimizer and loss\n",
    "model = NeuralNetwork(2,4,4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss = nn.BCELoss()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# train a neural network (feel free to choose the num_hidden1 and num_hidden2)\n",
    "# on the dataset in data.csv file\n",
    "# You'll have fun with conflicting shapes and types and tensors, but\n",
    "# you'll get those errors anyway. Let's go into the wild and learn\n",
    "# by reading the errors and trying to understand them! :)\n",
    "# You can always use the provided Workbook"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "df= pd.read_csv('data.csv',header=None)\n",
    "x = df.iloc[:,:-1].values\n",
    "y= df.iloc[:,-1]\n",
    "x = torch.tensor(x,dtype=torch.float)\n",
    "y=torch.tensor(y,dtype=torch.float).view(-1,1)\n",
    "def train(x,y,model,loss,epochs,lr):\n",
    "    for i in range(epochs):\n",
    "        optimizer.zero_grad() # making initial weight gradient for each epoch zero\n",
    "        pred = model(x) # prediction\n",
    "        loss_value = loss(pred,y) # calculation of the loss value\n",
    "        print(loss_value)\n",
    "        print(f'{i} epoch loss: {loss_value.item():.3f}')\n",
    "        loss_value.backward() # implementing the back prop, calculates change in loss func, due to change in weight\n",
    "        #which is gradient of that weight, stores to the leaf in dynamic graph\n",
    "        optimizer.step()#does single optimization"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "train(x,y,model,loss,100,0.01)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.1460, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "0 epoch loss: 0.146\n",
      "tensor(0.1460, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "1 epoch loss: 0.146\n",
      "tensor(0.1459, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "2 epoch loss: 0.146\n",
      "tensor(0.1459, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "3 epoch loss: 0.146\n",
      "tensor(0.1458, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "4 epoch loss: 0.146\n",
      "tensor(0.1457, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "5 epoch loss: 0.146\n",
      "tensor(0.1457, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "6 epoch loss: 0.146\n",
      "tensor(0.1456, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "7 epoch loss: 0.146\n",
      "tensor(0.1456, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "8 epoch loss: 0.146\n",
      "tensor(0.1455, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "9 epoch loss: 0.146\n",
      "tensor(0.1455, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "10 epoch loss: 0.145\n",
      "tensor(0.1455, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "11 epoch loss: 0.145\n",
      "tensor(0.1454, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "12 epoch loss: 0.145\n",
      "tensor(0.1454, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "13 epoch loss: 0.145\n",
      "tensor(0.1453, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "14 epoch loss: 0.145\n",
      "tensor(0.1453, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "15 epoch loss: 0.145\n",
      "tensor(0.1452, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "16 epoch loss: 0.145\n",
      "tensor(0.1452, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "17 epoch loss: 0.145\n",
      "tensor(0.1452, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "18 epoch loss: 0.145\n",
      "tensor(0.1451, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "19 epoch loss: 0.145\n",
      "tensor(0.1451, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "20 epoch loss: 0.145\n",
      "tensor(0.1450, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "21 epoch loss: 0.145\n",
      "tensor(0.1450, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "22 epoch loss: 0.145\n",
      "tensor(0.1450, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "23 epoch loss: 0.145\n",
      "tensor(0.1449, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "24 epoch loss: 0.145\n",
      "tensor(0.1449, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "25 epoch loss: 0.145\n",
      "tensor(0.1449, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "26 epoch loss: 0.145\n",
      "tensor(0.1448, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "27 epoch loss: 0.145\n",
      "tensor(0.1448, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "28 epoch loss: 0.145\n",
      "tensor(0.1448, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "29 epoch loss: 0.145\n",
      "tensor(0.1447, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "30 epoch loss: 0.145\n",
      "tensor(0.1447, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "31 epoch loss: 0.145\n",
      "tensor(0.1447, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "32 epoch loss: 0.145\n",
      "tensor(0.1446, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "33 epoch loss: 0.145\n",
      "tensor(0.1446, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "34 epoch loss: 0.145\n",
      "tensor(0.1446, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "35 epoch loss: 0.145\n",
      "tensor(0.1445, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "36 epoch loss: 0.145\n",
      "tensor(0.1445, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "37 epoch loss: 0.145\n",
      "tensor(0.1445, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "38 epoch loss: 0.144\n",
      "tensor(0.1445, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "39 epoch loss: 0.144\n",
      "tensor(0.1444, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "40 epoch loss: 0.144\n",
      "tensor(0.1444, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "41 epoch loss: 0.144\n",
      "tensor(0.1444, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "42 epoch loss: 0.144\n",
      "tensor(0.1444, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "43 epoch loss: 0.144\n",
      "tensor(0.1443, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "44 epoch loss: 0.144\n",
      "tensor(0.1443, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "45 epoch loss: 0.144\n",
      "tensor(0.1443, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "46 epoch loss: 0.144\n",
      "tensor(0.1443, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "47 epoch loss: 0.144\n",
      "tensor(0.1442, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "48 epoch loss: 0.144\n",
      "tensor(0.1442, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "49 epoch loss: 0.144\n",
      "tensor(0.1442, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "50 epoch loss: 0.144\n",
      "tensor(0.1442, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "51 epoch loss: 0.144\n",
      "tensor(0.1441, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "52 epoch loss: 0.144\n",
      "tensor(0.1441, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "53 epoch loss: 0.144\n",
      "tensor(0.1441, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "54 epoch loss: 0.144\n",
      "tensor(0.1441, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "55 epoch loss: 0.144\n",
      "tensor(0.1441, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "56 epoch loss: 0.144\n",
      "tensor(0.1440, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "57 epoch loss: 0.144\n",
      "tensor(0.1440, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "58 epoch loss: 0.144\n",
      "tensor(0.1440, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "59 epoch loss: 0.144\n",
      "tensor(0.1440, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "60 epoch loss: 0.144\n",
      "tensor(0.1440, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "61 epoch loss: 0.144\n",
      "tensor(0.1439, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "62 epoch loss: 0.144\n",
      "tensor(0.1439, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "63 epoch loss: 0.144\n",
      "tensor(0.1439, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "64 epoch loss: 0.144\n",
      "tensor(0.1439, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "65 epoch loss: 0.144\n",
      "tensor(0.1439, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "66 epoch loss: 0.144\n",
      "tensor(0.1439, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "67 epoch loss: 0.144\n",
      "tensor(0.1438, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "68 epoch loss: 0.144\n",
      "tensor(0.1438, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "69 epoch loss: 0.144\n",
      "tensor(0.1438, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "70 epoch loss: 0.144\n",
      "tensor(0.1438, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "71 epoch loss: 0.144\n",
      "tensor(0.1438, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "72 epoch loss: 0.144\n",
      "tensor(0.1438, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "73 epoch loss: 0.144\n",
      "tensor(0.1437, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "74 epoch loss: 0.144\n",
      "tensor(0.1437, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "75 epoch loss: 0.144\n",
      "tensor(0.1437, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "76 epoch loss: 0.144\n",
      "tensor(0.1437, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "77 epoch loss: 0.144\n",
      "tensor(0.1437, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "78 epoch loss: 0.144\n",
      "tensor(0.1437, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "79 epoch loss: 0.144\n",
      "tensor(0.1437, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "80 epoch loss: 0.144\n",
      "tensor(0.1436, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "81 epoch loss: 0.144\n",
      "tensor(0.1436, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "82 epoch loss: 0.144\n",
      "tensor(0.1436, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "83 epoch loss: 0.144\n",
      "tensor(0.1436, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "84 epoch loss: 0.144\n",
      "tensor(0.1436, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "85 epoch loss: 0.144\n",
      "tensor(0.1436, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "86 epoch loss: 0.144\n",
      "tensor(0.1436, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "87 epoch loss: 0.144\n",
      "tensor(0.1436, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "88 epoch loss: 0.144\n",
      "tensor(0.1435, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "89 epoch loss: 0.144\n",
      "tensor(0.1435, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "90 epoch loss: 0.144\n",
      "tensor(0.1435, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "91 epoch loss: 0.144\n",
      "tensor(0.1435, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "92 epoch loss: 0.144\n",
      "tensor(0.1435, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "93 epoch loss: 0.143\n",
      "tensor(0.1435, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "94 epoch loss: 0.143\n",
      "tensor(0.1435, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "95 epoch loss: 0.143\n",
      "tensor(0.1435, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "96 epoch loss: 0.143\n",
      "tensor(0.1434, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "97 epoch loss: 0.143\n",
      "tensor(0.1434, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "98 epoch loss: 0.143\n",
      "tensor(0.1434, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "99 epoch loss: 0.143\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('deep_env': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "interpreter": {
   "hash": "7b884b91808f249adbbbc046303483b453e9d806c558c4e2b2582ccec117f281"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}