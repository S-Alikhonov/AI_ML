{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from time import sleep"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "url scraper - after running imports, run cell below, it scraps all links\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "main_url = 'https://www.goodreads.com/list/show/50.The_Best_Epic_Fantasy_fiction_'\n",
    "driver = webdriver.Chrome()\n",
    "urls = []\n",
    "driver.get(main_url)\n",
    "\n",
    "for i in range(1,38):\n",
    "    if i ==2:\n",
    "        sleep(3)\n",
    "        x_button = driver.find_element_by_xpath('/html/body/div[3]/div/div/div[1]/button/img')\n",
    "        x_button.click()\n",
    "    book_table = driver.find_element_by_xpath('//table[@class=\"tableList js-dataTooltip\"]')\n",
    "    books = book_table.find_elements_by_tag_name('tr')\n",
    "\n",
    "    for book in books:\n",
    "        info = book.find_elements_by_tag_name('td')[2]\n",
    "        b_title = info.find_element_by_class_name('bookTitle')\n",
    "        urls.append(b_title.get_attribute('href'))\n",
    "    next_button = driver.find_element_by_xpath('//a[@class=\"next_page\"]')\n",
    "    try:\n",
    "        next_button.click()\n",
    "    except:\n",
    "        print(print('it was last one'))\n",
    "    \n",
    "    sleep(2)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "print(urls[720:740])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['https://www.goodreads.com/book/show/20821111-the-young-elites', 'https://www.goodreads.com/book/show/28374007-three-dark-crowns', 'https://www.goodreads.com/book/show/28796.The_Song_of_Rhiannon', 'https://www.goodreads.com/book/show/17157681-the-hobbit-part-one', 'https://www.goodreads.com/book/show/210197.Spirit_Gate', 'https://www.goodreads.com/book/show/92846.Antrax', 'https://www.goodreads.com/book/show/60145.Elric', 'https://www.goodreads.com/book/show/1442267.Elric_at_the_End_of_Time', 'https://www.goodreads.com/book/show/84950.Blood_and_Memory', 'https://www.goodreads.com/book/show/30981718-the-starbirth-assignment', 'https://www.goodreads.com/book/show/91477.Fool_Moon', 'https://www.goodreads.com/book/show/15557.Jarka_Ruus', 'https://www.goodreads.com/book/show/90619.Little_Big', 'https://www.goodreads.com/book/show/151242.Titus_Alone', 'https://www.goodreads.com/book/show/17671897-seven-forges', 'https://www.goodreads.com/book/show/32867216-shadow-team-gb', 'https://www.goodreads.com/book/show/91475.White_Night', 'https://www.goodreads.com/book/show/18966806-morning-star', 'https://www.goodreads.com/book/show/26032825-the-cruel-prince', 'https://www.goodreads.com/book/show/41637836-the-subtle-knife']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "scraper function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "def scraper(url_path):\n",
    "    titles = []\n",
    "    avg_ratings = []\n",
    "    authors = []\n",
    "    num_ratings = []\n",
    "    num_revs = []\n",
    "    num_pages = []\n",
    "    publ_years = []\n",
    "    is_series = []\n",
    "    genres = []\n",
    "    awards = []\n",
    "    places = []\n",
    "    pub_years_1 = []\n",
    "    book_urls = []\n",
    "    ################ starting point ############################\n",
    "    driver = webdriver.Chrome()\n",
    "    counter =0\n",
    "    pl_check = 0\n",
    "    place = ''\n",
    "    for url in url_path:\n",
    "        counter +=1\n",
    "        if counter %25==0:\n",
    "            print('{} books are scraped, 2 seconds of rest'.format(counter))\n",
    "            if counter%100==0:\n",
    "                sleep(3)\n",
    "            sleep(2)\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            box = driver.find_element_by_id('metacol')\n",
    "            ######## extracting info about title ##############\n",
    "            title = box.find_element_by_id('bookTitle').text\n",
    "            titles.append(title)\n",
    "            ######## extracting info about average rating ##############\n",
    "            author_box = box.find_element_by_id('bookAuthors')\n",
    "            author = author_box.find_elements_by_tag_name('a')[0].text\n",
    "            authors.append(author)\n",
    "            ######## extracting info about average rating ##############\n",
    "            rat_rev_box = box.find_element_by_id('bookMeta')\n",
    "            avg_rating = rat_rev_box.find_elements_by_tag_name('span')[6].text\n",
    "            avg_ratings.append(float(avg_rating))\n",
    "            ######## extracting info about number of ratings ##############\n",
    "            num_rating = rat_rev_box.find_elements_by_tag_name('a')[1].text.split()[0].replace(',','')\n",
    "            num_ratings.append(int(num_rating))\n",
    "            ######## extracting info about number of reviews ##############\n",
    "            num_rev = rat_rev_box.find_elements_by_tag_name('a')[2].text.split()[0].replace(',','')\n",
    "            num_revs.append(int(num_rev))\n",
    "            ########## finds element containing pages, pub year\n",
    "            details = box.find_element_by_id('details')\n",
    "            ######## is it aprt of book series ################\n",
    "            series = box.find_element_by_id('bookSeries')\n",
    "            if len(series.text)>0:\n",
    "                is_series.append(1)\n",
    "            else:\n",
    "                is_series.append(0)\n",
    "            ######## extracting info about pages ##############\n",
    "            page_check = 0\n",
    "            try:\n",
    "                pages = details.find_elements_by_tag_name('div')[0].text.split()\n",
    "                for i in pages:\n",
    "                    if i.isnumeric():\n",
    "                        num_pages.append(int(i))\n",
    "                        page_check=1\n",
    "                        break\n",
    "            except:\n",
    "                pass\n",
    "            if not page_check:\n",
    "                num_pages.append(None)\n",
    "            ####### extracting info about pub year ##############\n",
    "            year_check = 0\n",
    "            try:\n",
    "                years_info = details.find_elements_by_tag_name('div')[1]\n",
    "                pub_years = years_info.text.split()\n",
    "                for year in pub_years:\n",
    "                    if len(year)==4 and year.isnumeric():\n",
    "                        publ_years.append(int(year))\n",
    "                        year_check = 1\n",
    "            except:\n",
    "                pass\n",
    "            if not year_check:\n",
    "                publ_years.append(None)\n",
    "            ####### extracting info about pub year first ##############\n",
    "            year_check_1 = 0\n",
    "            try:\n",
    "                pub_years_first = years_info.find_elements_by_class_name('greyText')[0].text.replace('(','').replace(')','').split()\n",
    "                for yr in pub_years_first:\n",
    "                    if len(yr)==4 and yr.isnumeric():\n",
    "                        pub_years_1.append(int(yr))\n",
    "                        year_check_1 = 1\n",
    "            except:\n",
    "                pass\n",
    "            if not year_check_1:\n",
    "                pub_years_1.append(None)\n",
    "            ######## extracting info about genre ##############\n",
    "            c = 0\n",
    "            try:\n",
    "                genres_box = driver.find_element_by_xpath('//div[@class=\"stacked\"]').find_element_by_xpath('.//div[@class=\"bigBoxContent containerWithHeaderContent\"]').find_elements_by_class_name('elementList')[:3]\n",
    "                genre_complete=''\n",
    "                for genre in genres_box:\n",
    "                    genre_complete+=',{}'.format(genre.find_elements_by_tag_name('div')[0].text)\n",
    "                genres.append(genre_complete[1:])\n",
    "                c=1\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "            if not c:\n",
    "                genres.append(None)\n",
    "            ######## extracting info about awards ##############\n",
    "            more_info_button = details.find_element_by_id('bookDataBoxShow')\n",
    "            try:\n",
    "                more_info_button.click()\n",
    "            except:\n",
    "                pass\n",
    "            check = 0\n",
    "\n",
    "            award_places = details.find_elements_by_tag_name('div')[2].find_elements_by_tag_name('div')[0]\n",
    "            try:\n",
    "                award_box = award_places.find_elements_by_class_name('clearFloats')\n",
    "                for box in award_box:\n",
    "                    check = 0\n",
    "                    try:\n",
    "                        award = box.find_element_by_xpath('.//div[@itemprop=\"awards\"]')\n",
    "                        check = 1\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "            if not check:\n",
    "                awards.append(None)\n",
    "            else:\n",
    "                awards.append(award.text)\n",
    "                check=0\n",
    "            ####### extracting info about places ##############\n",
    "            settings = award_places.find_elements_by_class_name('infoBoxRowItem')\n",
    "            for setting in settings:\n",
    "                plcs = setting.find_elements_by_tag_name('a')\n",
    "                for plc in plcs:\n",
    "                    linking = plc.get_attribute('href')\n",
    "                    if '/places/' in linking:\n",
    "                        place += ',{}'.format(plc.text)\n",
    "                        pl_check =1\n",
    "\n",
    "            if not pl_check:\n",
    "                places.append(None)\n",
    "            else:\n",
    "                places.append(place[1:])\n",
    "                place=''\n",
    "            pl_check = 0\n",
    "            book_urls.append(url)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    data = {\n",
    "    \"title\":titles,\n",
    "    \"author\":authors,\n",
    "    \"num_reviews\":num_revs,\n",
    "    \"num_ratings\":num_ratings,\n",
    "    \"avg_rating\":avg_ratings,\n",
    "    \"num_pages\":num_pages,\n",
    "    \"publish_year\":publ_years,\n",
    "    \"first_published\":pub_years_1,\n",
    "    \"series\":is_series,\n",
    "    \"genres\":genres,\n",
    "    \"awards\":awards,\n",
    "    \"places\":places,\n",
    "    \"url\": book_urls\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Frame creation - as you can see, i provided urls as argument to the scraper function. modifying indexing values, run cell below\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = scraper(urls[2500:3000])\n",
    "df = pd.DataFrame(data,dtype=object)\n",
    "df.to_csv('2500:3000.csv')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "25 books are scraped, 2 seconds of rest\n",
      "50 books are scraped, 2 seconds of rest\n",
      "75 books are scraped, 2 seconds of rest\n",
      "100 books are scraped, 2 seconds of rest\n",
      "125 books are scraped, 2 seconds of rest\n",
      "150 books are scraped, 2 seconds of rest\n",
      "175 books are scraped, 2 seconds of rest\n",
      "200 books are scraped, 2 seconds of rest\n",
      "225 books are scraped, 2 seconds of rest\n",
      "250 books are scraped, 2 seconds of rest\n",
      "275 books are scraped, 2 seconds of rest\n",
      "300 books are scraped, 2 seconds of rest\n",
      "325 books are scraped, 2 seconds of rest\n",
      "350 books are scraped, 2 seconds of rest\n",
      "375 books are scraped, 2 seconds of rest\n",
      "400 books are scraped, 2 seconds of rest\n",
      "425 books are scraped, 2 seconds of rest\n",
      "450 books are scraped, 2 seconds of rest\n",
      "475 books are scraped, 2 seconds of rest\n",
      "500 books are scraped, 2 seconds of rest\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "print(len(data['places']))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "160\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "after scraping, to save it as csv file, run cell below"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('sayeed': conda)"
  },
  "interpreter": {
   "hash": "57ccbecd372b9becaf4e59334636d42e5a64583ce917e4ad46b635842aed01d7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}