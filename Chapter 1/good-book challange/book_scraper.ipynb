{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "run this cell to **scrape urls** of the books."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "driver = webdriver.Chrome()\n",
    "def link_scraper(main_url,final_page):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    this function takes url (string) and final_page(integer)\n",
    "    srapes individual book url, targeting the href attribute of the book.\n",
    "    returns the resulst, as a list of urls, scraped till the page user input as an argument\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    urls = []\n",
    "    for i in range(final_page):\n",
    "        url = main_url+\"page={}\".format(i+1)\n",
    "        driver.get(url)\n",
    "        book_table = driver.find_element_by_xpath('//table[@class=\"tableList js-dataTooltip\"]')#targets the box that contains the list of books\n",
    "        books = book_table.find_elements_by_tag_name('tr')#targets each and every book\n",
    "        for book in books:\n",
    "            info = book.find_elements_by_tag_name('td')[2]#targets the box containing info about the book\n",
    "            b_title = info.find_element_by_class_name('bookTitle')#targets title of the book, and extracts href as url from it\n",
    "            urls.append(b_title.get_attribute('href'))\n",
    "        sleep(1)\n",
    "    return urls\n",
    "\n",
    "urls = link_scraper('https://www.goodreads.com/list/show/50.The_Best_Epic_Fantasy_fiction_?',37)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "run this **to scrape** the data of an individual book\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import pandas as pd\n",
    "def scraper(url_path):\n",
    "    \"\"\"\n",
    "    scraper function takes list of urls (strings) as an argument\n",
    "    targets the title, average rating, number of ratings, number of reviews,\n",
    "    number of pages, publication year and first publication year, whether book belongs\n",
    "    to the book series, genres, awards.\n",
    "\n",
    "    extracts those values, stores into separate lists, then creates the dictionary from them.\n",
    "    Returns the result as a dictionary.\n",
    "\n",
    "    \"\"\"\n",
    "    titles = []\n",
    "    avg_ratings = []\n",
    "    authors = []\n",
    "    num_ratings = []\n",
    "    num_revs = []\n",
    "    num_pages = []\n",
    "    publ_years = []\n",
    "    is_series = []\n",
    "    genres = []\n",
    "    awards = []\n",
    "    places = []\n",
    "    pub_years_1 = []\n",
    "    book_urls = []\n",
    "    ################ starting point ############################\n",
    "    driver = webdriver.Chrome()\n",
    "    counter = 0 # counter used to count number of books scraped\n",
    "    pl_check = 0\n",
    "    place = ''\n",
    "    for url in url_path:\n",
    "        counter +=1 # counter increment\n",
    "        if counter %25==0: #when each 25 pile scraped, it prints and sleeps for 2 secs\n",
    "            print('{} books are scraped, 2 seconds of rest'.format(counter))\n",
    "            if counter%100==0:\n",
    "                sleep(3) #it sleep extra 3 secs, when a 100 books' pile are done\n",
    "            sleep(2)\n",
    "        driver.get(url)#loads the each individual page\n",
    "        try:\n",
    "            box = driver.find_element_by_id('metacol')\n",
    "            ######## extracting info about title ##############\n",
    "            title = box.find_element_by_id('bookTitle').text #targets unique item with id \n",
    "            titles.append(title)\n",
    "            ######## extracting info about average rating ##############\n",
    "            author_box = box.find_element_by_id('bookAuthors') #targets box containing author info\n",
    "            author = author_box.find_elements_by_tag_name('a')[0].text\n",
    "            authors.append(author)\n",
    "            ######## extracting info about average rating ##############\n",
    "            rat_rev_box = box.find_element_by_id('bookMeta') # targets the box containing info about ratings, reviews\n",
    "            avg_rating = rat_rev_box.find_elements_by_tag_name('span')[6].text#Targets the 7th span as it holds avg_rating info\n",
    "            avg_ratings.append(float(avg_rating))\n",
    "            ######## extracting info about number of ratings ##############\n",
    "            num_rating = rat_rev_box.find_elements_by_tag_name('a')[1].text.split()[0].replace(',','')\n",
    "            num_ratings.append(int(num_rating))\n",
    "            ######## extracting info about number of reviews ##############\n",
    "            num_rev = rat_rev_box.find_elements_by_tag_name('a')[2].text.split()[0].replace(',','')\n",
    "            num_revs.append(int(num_rev))\n",
    "            ########## finds element containing pages, pub year\n",
    "            details = box.find_element_by_id('details') #targets details box containing info about a book\n",
    "            ######## is it aprt of book series ################\n",
    "            series = box.find_element_by_id('bookSeries')\n",
    "            if len(series.text)>0: #checks whether bookSeries text exists or not\n",
    "                is_series.append(1)\n",
    "            else:\n",
    "                is_series.append(0)\n",
    "            ######## extracting info about pages ##############\n",
    "            page_check = 0\n",
    "            try:\n",
    "                #targets item with itemrop = 'numberOfPages'\n",
    "                pages = details.find_elements_by_tag_name('div')[0].find_element_by_xpath('.//span[@itemprop=\"numberOfPages\"]').text.split()\n",
    "                for i in pages:\n",
    "                    if i.isnumeric(): #check if it is numeric\n",
    "                        num_pages.append(int(i))\n",
    "                        page_check=1 #make check mask 1\n",
    "                        break\n",
    "            except:\n",
    "                pass\n",
    "            if not page_check: #if mask never change, the value will be None\n",
    "                num_pages.append(None)\n",
    "            ####### extracting info about pub year ##############\n",
    "            year_check = 0\n",
    "            try:\n",
    "                years_info = details.find_elements_by_tag_name('div')[1]\n",
    "                pub_years = years_info.text.split()\n",
    "                for year in pub_years:\n",
    "                    if len(year)==4 and year.isnumeric():#if it has length of 4 and integer, it will be a year\n",
    "                        publ_years.append(int(year))\n",
    "                        year_check = 1\n",
    "            except:\n",
    "                pass\n",
    "            if not year_check:\n",
    "                publ_years.append(None)\n",
    "            ####### extracting info about pub year first ##############\n",
    "            year_check_1 = 0\n",
    "            try:\n",
    "                pub_years_first = years_info.find_elements_by_class_name('greyText')[0].text.replace('(','').replace(')','').split()\n",
    "                for yr in pub_years_first:\n",
    "                    if len(yr)==4 and yr.isnumeric():\n",
    "                        pub_years_1.append(int(yr))\n",
    "                        year_check_1 = 1\n",
    "            except:\n",
    "                pass\n",
    "            if not year_check_1:\n",
    "                pub_years_1.append(None)\n",
    "            ######## extracting info about genre ##############\n",
    "            genre_check = 0\n",
    "            try:\n",
    "                genres_box = driver.find_element_by_xpath('//div[@class=\"stacked\"]').\\\n",
    "                    find_element_by_xpath('.//div[@class=\"bigBoxContent containerWithHeaderContent\"]').\\\n",
    "                    find_elements_by_class_name('elementList')[:3] #targets first 3 genre items\n",
    "                genre_complete=''\n",
    "                \n",
    "                for genre in genres_box: #extracts text and merges them\n",
    "                    genre_complete+=',{}'.format(genre.find_elements_by_tag_name('div')[0].text)\n",
    "                genres.append(genre_complete[1:])\n",
    "                genre_check=1\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "            if not genre_check :\n",
    "                genres.append(None)\n",
    "            ######## extracting info about awards ##############\n",
    "            more_info_button = details.find_element_by_id('bookDataBoxShow')\n",
    "            try:\n",
    "                more_info_button.click()\n",
    "            except:\n",
    "                pass\n",
    "            check = 0\n",
    "\n",
    "            award_places = details.find_elements_by_tag_name('div')[2].find_elements_by_tag_name('div')[0]\n",
    "            try: #comment out from here else included, if you want aslo click more... button and scrape elements from there\n",
    "                award_box = award_places.find_elements_by_class_name('clearFloats')\n",
    "                for box in award_box:\n",
    "                    check = 0\n",
    "                    try: # targets on item with itemprop = 'awards'\n",
    "                        award = box.find_element_by_xpath('.//div[@itemprop=\"awards\"]')\n",
    "                        check = 1\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "            if not check:\n",
    "                awards.append(None)\n",
    "            else:\n",
    "                awards.append(award.text)\n",
    "                check=0\n",
    "\n",
    "            ### !!! then uncomment code below\n",
    "\n",
    "            # try:\n",
    "            #     award_box = award_places.find_elements_by_class_name('clearFloats')\n",
    "            #     for box in award_box:\n",
    "            #         check = 0\n",
    "            #         try:\n",
    "            #             award = box.find_element_by_xpath('.//div[@itemprop=\"awards\"]')\n",
    "            #             more_bttn = award.find_elements_by_tag_name('span')[0].find_elements_by_class_name('actionLinkLite')[0]\n",
    "            #             print('clicked')\n",
    "            #             more_bttn.click()\n",
    "            #             check = 1\n",
    "            #         except:\n",
    "            #             pass\n",
    "            # except:\n",
    "            #     pass\n",
    "            # if not check:\n",
    "            #     awards.append(None)\n",
    "            # else:\n",
    "            #     awards.append(award.text.replace('...less',''))\n",
    "            #     check=0\n",
    "            ####### extracting info about places ##############\n",
    "            settings = award_places.find_elements_by_class_name('infoBoxRowItem')\n",
    "            for setting in settings:\n",
    "                plcs = setting.find_elements_by_tag_name('a')\n",
    "                for plc in plcs:\n",
    "                    linking = plc.get_attribute('href')#extracts href attribute\n",
    "                    if '/places/' in linking: #check if it contains /places/\n",
    "                        place += ',{}'.format(plc.text)\n",
    "                        pl_check =1\n",
    "\n",
    "            if not pl_check:\n",
    "                places.append(None)\n",
    "            else:\n",
    "                places.append(place[1:])\n",
    "                place=''\n",
    "            pl_check = 0\n",
    "            book_urls.append(url)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    #creates dictionary out of lists, then returns it\n",
    "    data = {\n",
    "    \"title\":titles,\n",
    "    \"author\":authors,\n",
    "    \"num_reviews\":num_revs,\n",
    "    \"num_ratings\":num_ratings,\n",
    "    \"avg_rating\":avg_ratings,\n",
    "    \"num_pages\":num_pages,\n",
    "    \"publish_year\":publ_years,\n",
    "    \"first_published\":pub_years_1,\n",
    "    \"series\":is_series,\n",
    "    \"genres\":genres,\n",
    "    \"awards\":awards,\n",
    "    \"places\":places,\n",
    "    \"url\": book_urls\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "25 books are scraped, 2 seconds of rest\n",
      "50 books are scraped, 2 seconds of rest\n",
      "75 books are scraped, 2 seconds of rest\n",
      "100 books are scraped, 2 seconds of rest\n",
      "125 books are scraped, 2 seconds of rest\n",
      "150 books are scraped, 2 seconds of rest\n",
      "175 books are scraped, 2 seconds of rest\n",
      "200 books are scraped, 2 seconds of rest\n",
      "225 books are scraped, 2 seconds of rest\n",
      "250 books are scraped, 2 seconds of rest\n",
      "275 books are scraped, 2 seconds of rest\n",
      "300 books are scraped, 2 seconds of rest\n",
      "325 books are scraped, 2 seconds of rest\n",
      "350 books are scraped, 2 seconds of rest\n",
      "375 books are scraped, 2 seconds of rest\n",
      "400 books are scraped, 2 seconds of rest\n",
      "425 books are scraped, 2 seconds of rest\n",
      "450 books are scraped, 2 seconds of rest\n",
      "475 books are scraped, 2 seconds of rest\n",
      "500 books are scraped, 2 seconds of rest\n",
      "525 books are scraped, 2 seconds of rest\n",
      "550 books are scraped, 2 seconds of rest\n",
      "575 books are scraped, 2 seconds of rest\n",
      "600 books are scraped, 2 seconds of rest\n",
      "625 books are scraped, 2 seconds of rest\n",
      "650 books are scraped, 2 seconds of rest\n",
      "675 books are scraped, 2 seconds of rest\n",
      "700 books are scraped, 2 seconds of rest\n",
      "725 books are scraped, 2 seconds of rest\n",
      "750 books are scraped, 2 seconds of rest\n",
      "775 books are scraped, 2 seconds of rest\n",
      "800 books are scraped, 2 seconds of rest\n",
      "825 books are scraped, 2 seconds of rest\n",
      "850 books are scraped, 2 seconds of rest\n",
      "875 books are scraped, 2 seconds of rest\n",
      "900 books are scraped, 2 seconds of rest\n",
      "925 books are scraped, 2 seconds of rest\n",
      "950 books are scraped, 2 seconds of rest\n",
      "975 books are scraped, 2 seconds of rest\n",
      "1000 books are scraped, 2 seconds of rest\n",
      "1025 books are scraped, 2 seconds of rest\n",
      "1050 books are scraped, 2 seconds of rest\n",
      "1075 books are scraped, 2 seconds of rest\n",
      "1100 books are scraped, 2 seconds of rest\n",
      "1125 books are scraped, 2 seconds of rest\n",
      "1150 books are scraped, 2 seconds of rest\n",
      "1175 books are scraped, 2 seconds of rest\n",
      "1200 books are scraped, 2 seconds of rest\n",
      "1225 books are scraped, 2 seconds of rest\n",
      "1250 books are scraped, 2 seconds of rest\n",
      "1275 books are scraped, 2 seconds of rest\n",
      "1300 books are scraped, 2 seconds of rest\n",
      "1325 books are scraped, 2 seconds of rest\n",
      "1350 books are scraped, 2 seconds of rest\n",
      "1375 books are scraped, 2 seconds of rest\n",
      "1400 books are scraped, 2 seconds of rest\n",
      "1425 books are scraped, 2 seconds of rest\n",
      "1450 books are scraped, 2 seconds of rest\n",
      "1475 books are scraped, 2 seconds of rest\n",
      "1500 books are scraped, 2 seconds of rest\n",
      "1525 books are scraped, 2 seconds of rest\n",
      "1550 books are scraped, 2 seconds of rest\n",
      "1575 books are scraped, 2 seconds of rest\n",
      "1600 books are scraped, 2 seconds of rest\n",
      "1625 books are scraped, 2 seconds of rest\n",
      "1650 books are scraped, 2 seconds of rest\n",
      "1675 books are scraped, 2 seconds of rest\n",
      "1700 books are scraped, 2 seconds of rest\n",
      "1725 books are scraped, 2 seconds of rest\n",
      "1750 books are scraped, 2 seconds of rest\n",
      "1775 books are scraped, 2 seconds of rest\n",
      "1800 books are scraped, 2 seconds of rest\n",
      "1825 books are scraped, 2 seconds of rest\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "run cell below to create **DataFrame** and **save it as CSV**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "\n",
    "def dataframer(starting,ending,urls):\n",
    "\n",
    "    \"\"\"\n",
    "    dataframer function takes starting(int)-starting idex of slicing, \n",
    "    ending(int)-ending index of slicing, list of urls (list)\n",
    "    then calls scraper function, to get the dictionary.\n",
    "    Then creates dataset from it using pandas DataFrame.\n",
    "    Stores this dataset as csv file into the hard drive of local machine.\n",
    "    \n",
    "    \"\"\"\n",
    "    data = scraper(['https://www.goodreads.com/book/show/5293561-dragonfly-in-amber'])\n",
    "    df = pd.DataFrame(data,dtype=object)\n",
    "    df.to_csv('{}:{}_books.csv'.format(starting,ending))\n",
    "\n",
    "dataframer(1800,1900,urls)\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('sayeed': conda)"
  },
  "interpreter": {
   "hash": "57ccbecd372b9becaf4e59334636d42e5a64583ce917e4ad46b635842aed01d7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}